{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Beer Recommendation System\n",
    "\n",
    "Description: `beer_data.csv` :  Each record includes a beer's name and the user's name, along with the ratings he/she has given to the beer. All ratings are on a scale from 1 to 5, with 5 being the best rating.\n",
    "\n",
    "### Purpose of the Case study :\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "Choose only those beers that have at least N number of reviews. (Figure out an appropriate value of N using EDA)\n",
    "\n",
    "#### Data exploration\n",
    "\n",
    "1) What are the unique values of ratings? <br>\n",
    "2) Visualise the rating values and notice:<br>\n",
    "    a) The average beer ratings<br>\n",
    "    b) The average user ratings<br>\n",
    "    c) The average number of ratings given to the beers<br>\n",
    "    d) The average number of ratings given by the users<br>\n",
    "\n",
    "#### Recommendation Models\n",
    "\n",
    "1) Divide your data into training and testing dataset.<br>\n",
    "2) Build user-based and item-based models.<br>\n",
    "3) Determine how similar the first 10 users are to each other and visualise it.<br>\n",
    "4) Compute and visualise the similarity between the first 10 beers.<br>\n",
    "5) Compare the performance of the two models using test data and suggest the one that should be deployed.<br>\n",
    "6) Give the names of the top 5 beers that you would recommend to the users 'cokes', 'genog' and 'giblet' using both the models.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# let's import the scaling libraries, Since lot of dummy variables we will use MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading ratings file\n",
    "ratings = pd.read_csv('beer_data.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since column count is usually more than the default Jupyter settings, let's refit the visible columns\n",
    "pd.set_option('max_columns', 99999)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "#### 1) Let's check true duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dup = ratings[ratings.duplicated(subset=['beer_beerid','review_profilename'], keep=\"last\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.loc[(ratings['review_profilename'] == 'AleWatcher' )\n",
    "            & (ratings['beer_beerid'] == 52211 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dup = ratings[ratings.duplicated(subset=['beer_beerid','review_profilename'], keep=\"last\")]\n",
    "print ('Original file dataframe:', ratings.shape , '; Duplicate dataframe:', ratings_dup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.loc[(ratings['review_profilename'] == 'RedDiamond' )\n",
    "            & (ratings['beer_beerid'] == 962 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.loc[(ratings['review_profilename'] == 'barleywinefiend' )\n",
    "            & (ratings['beer_beerid'] == 73647 ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop these true duplicates \n",
    "1) Assumption - same user giving two reviews, last review is expected to be latest and considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dup = ratings[ratings.duplicated(subset=['beer_beerid','review_profilename'], keep=\"last\")]\n",
    "ratings_non_dup = ratings[~ratings.duplicated(subset=['beer_beerid','review_profilename'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dup data frame :',ratings_dup.shape)\n",
    "print('Non-Dup data frame: ',ratings_non_dup.shape)\n",
    "print('Rows excluding Dup data :',ratings.shape[0] - ratings_non_dup.shape[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_fin = pd.concat([ratings_dup, ratings_non_dup], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification post removal of true duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_fin.loc[(ratings_fin['review_profilename'] == 'AleWatcher' )\n",
    "            & (ratings_fin['beer_beerid'] == 52211 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_fin.loc[(ratings_fin['review_profilename'] == 'RedDiamond' )\n",
    "            & (ratings_fin['beer_beerid'] == 962 ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We no longer have `true duplicates`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Check NaN or null values if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check percentage of null values, insert the same onto dataframe, choose the columns where % null values are > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(round(100*(ratings_fin.isnull().sum()/len(ratings_fin.index)), 2)).reset_index()\n",
    "a.columns = ['column_name', 'null_pct']\n",
    "a = a.loc[ (a['null_pct'] > 0) , :]\n",
    "a.sort_values(by='null_pct', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Very few % of null values across different rows. We won't be imputing value for these columns and will drop rows. Once done, the total % is calculated to see if this has any impact on the total % data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_nan = ratings_fin.dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1= ratings_fin.shape[0]\n",
    "r2= ratings_nan.shape[0]\n",
    "pct_dropped = 100*(r1-r2)/r1\n",
    "pct_avail = 100 - 100* (r1-r2)/r1\n",
    "print(\"% of records dropped :\", format(pct_dropped))\n",
    "print(\"% of records available :\", format(pct_avail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN\n",
    "ratings = ratings_fin.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(round(100*(ratings.isnull().sum()/len(ratings.index)), 2)).reset_index()\n",
    "a.columns = ['column_name', 'null_pct']\n",
    "a = a.loc[ (a['null_pct'] > 0) , :]\n",
    "a.sort_values(by='null_pct', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No more Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of records after cleaning duplicates and NaN:', ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Let's perform EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's group by beer_beerid and bin the total number of reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ratings.groupby(['beer_beerid']).count()\n",
    "rb.sort_values('review_overall', ascending=False)\n",
    "\n",
    "bins = [0, 1, 5, 10, 50, 100, 500, 1000, 5000]\n",
    "print(rb.groupby(pd.cut(rb['review_overall'], bins=bins)).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratings less than 10 for any given beer appears to be very less considering that 50+ beers have > 500 reviews. We choose K as 10 as that we have a decent number of ratings per beer before we go on to build the model. Let's derive percentile and plot histogram before applying the filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's apply filter. From grouped by data, we choose reviews > 10 per beer. We then set index on the main dataframe/ grouped dataframe on beer_id then use isin to apply filter on the main dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some of the highest reviews obtained for any beer_beerid\n",
    "rb = ratings.groupby(['beer_beerid']).count()\n",
    "rb.sort_values('review_overall', ascending= False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the lowest reviews obtained for any beer_beerid (single reviews)\n",
    "rb.sort_values('review_overall', ascending= True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_beers = pd.DataFrame(rb.loc[rb['review_overall'] > 10 , :] ).reset_index()\n",
    "selected_beers.columns = ['beer_beerid', 'review_profilename' , 'review_overall']\n",
    "print(selected_beers.shape)\n",
    "i1 = ratings.set_index('beer_beerid').index\n",
    "i2 = selected_beers.set_index('beer_beerid').index\n",
    "tmp_df = ratings[i1.isin(i2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Dataframe :', ratings.shape)\n",
    "print('After removing low rated beers(with N=2 derived as median) Dataframe :', tmp_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we exclude individuals who just rated  beers < 5. This is done so that we don't consider those reviewers who haven't contributed a whole lot for the reviews. Count of 5 is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = tmp_df.groupby(['review_profilename']).count()\n",
    "rb.sort_values('review_overall', ascending=False)\n",
    "\n",
    "bins = [0, 1, 5, 10, 50, 100, 500, 1000, 5000]\n",
    "print(rb.groupby(pd.cut(rb['review_overall'], bins=bins)).size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We don't want to consider those who rated less than 5 beers , 5 seems to be optimum cutoff to rule out any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_beer = tmp_df.groupby(['review_profilename']).count()\n",
    "profile_beer.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_profiles = pd.DataFrame(profile_beer.loc[profile_beer['beer_beerid'] > 5 , :] ).reset_index()\n",
    "selected_profiles.columns = ['review_profilename' , 'beer_beerid',  'review_overall']\n",
    "i1 = tmp_df.set_index('review_profilename').index\n",
    "i2 = selected_profiles.set_index('review_profilename').index\n",
    "ratings = tmp_df[i1.isin(i2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Dataframe :', tmp_df.shape)\n",
    "print('After removing <10 review contributions, Dataframe :', ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) What are the unique values of ratings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(ratings.groupby(['review_overall'])['beer_beerid'].count()).reset_index()\n",
    "a.columns = ['review_overall', 'count']\n",
    "a.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.groupby(['review_overall'])['beer_beerid'].count().plot(kind='bar', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique values for ratings are :', list(a['review_overall']))\n",
    "# unique ratings are [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Visualise the rating values and notice: <br>\n",
    "a) The average beer ratings <br>\n",
    "b) The average user ratings <br>\n",
    "c) The average number of ratings given to the beers <br>\n",
    "d) The average number of ratings given by the users <br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) The average beer ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(ratings.groupby(['beer_beerid'])['review_overall'].mean()).reset_index()\n",
    "a.columns = ['review_overall', 'average']\n",
    "print ('** Top 5 average beer ratings with beer_beerid ** \\n',\n",
    "       a.sort_values(by='average', ascending=False).head(5))\n",
    "\n",
    "print('** Bottom 5 average beer ratings with beer_beerid ** \\n',\n",
    "      a.sort_values(by='average', ascending=True).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of The average beer ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "sns.distplot(a['average'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average beer ratings:' , a['average'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)The average user ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(ratings.groupby(['review_profilename'])['review_overall'].mean()).reset_index()\n",
    "a.columns = ['review_overall', 'average']\n",
    "print ('** Top 5 reviewers with average ratings based on review_profilename ** \\n',\n",
    "       a.sort_values(by='average', ascending=False).head(5))\n",
    "\n",
    "print('** Bottom 5 reviewers with average ratings based on review_profilename  ** \\n',\n",
    "      a.sort_values(by='average', ascending=True).head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of The average user ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "sns.distplot(a['average'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average user ratings :',a['average'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) The average number of ratings given to the beers** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(ratings.groupby(['beer_beerid'])['review_overall'].count()).reset_index()\n",
    "a.columns = ['review_overall', 'count']\n",
    "print ('**Top 5 number of beer ratings with beer_beerid ** \\n', a.sort_values(by='count', ascending=False).head(5))\n",
    "print('**Bottom 5 number of beer ratings with beer_beerid ** \\n', a.sort_values(by='count', ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "sns.distplot(a['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average number of ratings given to the beers :',a['count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) The average number of ratings given by the users** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(ratings.groupby(['review_profilename'])['review_overall'].count()).reset_index()\n",
    "a.columns = ['review_overall', 'count']\n",
    "print ('** Top 5 reviewers with review_profilename ** \\n', a.sort_values(by='count', ascending=False).head(5))\n",
    "print('** Bottom 5 reviewers with review_profilename ** \\n', a.sort_values(by='count', ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "sns.distplot(a['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average number of ratings given by the users  :',a['count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Models\n",
    "\n",
    "1) Divide your data into training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset size before we begin split into test/train: ' , ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(ratings, test_size=0.30, random_state=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since dataset is large, we don't need to stratify the sample**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Build user-based and item-based models. <br>\n",
    "2a) User-based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfuser = pd.pivot_table(train,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'],fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfuser.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy train and test dataset\n",
    "These dataset will be used for prediction and evaluation. \n",
    "- Dummy train will be used later for prediction of the beers which has not been rated by the user. To ignore the beers rated by the user, we will mark it as 0 during prediction. The beers not rated by user is marked as 1 for prediction. \n",
    "- Dummy test will be used for evaluation. To evaluate, we will only make prediction on the beers rated by the user. So, this is marked as 1. This is just opposite of dummy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = train.copy()\n",
    "dummy_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train['review_overall'] = dummy_train['review_overall'].apply(lambda x: 0 if x>=1 else 1)\n",
    "dummy_test['review_overall'] = dummy_test['review_overall'].apply(lambda x: 1 if x>=1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The movies not rated by user is marked as 1 for prediction. \n",
    "dummy_train =  pd.pivot_table(dummy_train,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'],fill_value=1)\n",
    "\n",
    "dummy_test =  pd.pivot_table(dummy_test,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'],fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using similarity matrix - Cosine similarity**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(dfuser, metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using adjusted Cosine - not removing the NaN values and calculating the mean only for the beers rated by the user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfuser_w_nan = pd.pivot_table(train,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfuser_w_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalising the rating of the Beers for each user aroung 0 mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.nanmean(dfuser_w_nan, axis=1)\n",
    "df_subtracted = (dfuser_w_nan.T-mean).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtracted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's find Cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "Doing the prediction for the users which are positively related with other users, and not the users which are negatively related as we are interested in the users which are more similar to the current users. So, ignoring the correlation for values less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation[user_correlation<0]=0\n",
    "user_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating predicted by the user (for beers rated as well as not rated) is the weighted sum of correlation with the beer ratings (as present in the rating dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings = np.dot(user_correlation, dfuser_w_nan.fillna(0))\n",
    "user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_final_rating = np.multiply(user_predicted_ratings,dummy_train)\n",
    "user_final_rating.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find top 5 recommendations for user1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_final_rating.iloc[0].sort_values(ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b)Item based model**\n",
    "\n",
    "Using Correlation\n",
    "\n",
    "Taking the transpose of the rating matrix to normalize the rating around the mean for different beer_beerid. In the user based similarity, we had taken mean for each user intead of each beerid reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfitem = pd.pivot_table(train,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'],fill_value=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfitem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising the rating for beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.nanmean(dfitem, axis=1)\n",
    "df_subtracted = (dfitem.T-mean).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtracted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine similarity** <br>\n",
    "Note that since the data is normalised, both the cosine metric and correlation metric will give the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# User Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's choose positive correlation\n",
    "item_correlation[item_correlation<0]=0\n",
    "item_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predicted_ratings = np.dot((dfitem.fillna(0).T),item_correlation)\n",
    "item_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering the rating only for the beers not rated by the users for recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_final_rating = np.multiply(item_predicted_ratings,dummy_train)\n",
    "item_final_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top5 item prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_final_rating.iloc[1].sort_values(ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Compare the performance of the two models using test data and suggest the one that should be deployed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_dfuser = pd.pivot_table(test,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'])\n",
    "\n",
    "mean = np.nanmean(T_dfuser, axis=1)\n",
    "test_df_subtracted = (T_dfuser.T-mean).T\n",
    "\n",
    "# User Similarity Matrix\n",
    "test_user_correlation = 1 - pairwise_distances(test_df_subtracted.fillna(0), metric='cosine')\n",
    "test_user_correlation[np.isnan(test_user_correlation)] = 0\n",
    "print(test_user_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_correlation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_correlation[test_user_correlation<0]=0\n",
    "test_user_predicted_ratings = np.dot(test_user_correlation, T_dfuser.fillna(0))\n",
    "test_user_predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_final_rating = np.multiply(test_user_predicted_ratings,dummy_test)\n",
    "test_user_final_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Root Mean Square error/RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "\n",
    "X  = test_user_final_rating.copy() \n",
    "X = X[X>0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.pivot_table(test,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding total non-NaN value\n",
    "total_non_nan = np.count_nonzero(~np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (sum(sum((test_ - y )**2))/total_non_nan)**0.5\n",
    "print('RMSE for User-based model :  ',rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Item Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_dfitem = pd.pivot_table(test,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid']).T\n",
    "\n",
    "\n",
    "mean = np.nanmean(T_dfitem, axis=1)\n",
    "test_df_subtracted = (T_dfitem.T-mean).T\n",
    "\n",
    "test_item_correlation = 1 - pairwise_distances(test_df_subtracted.fillna(0), metric='cosine')\n",
    "test_item_correlation[np.isnan(test_item_correlation)] = 0\n",
    "test_item_correlation[test_item_correlation<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_correlation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_dfitem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_predicted_ratings = (np.dot(test_item_correlation, T_dfitem.fillna(0))).T\n",
    "test_item_final_rating = np.multiply(test_item_predicted_ratings,dummy_test)\n",
    "test_item_final_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.pivot_table(test,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "\n",
    "X  = test_item_final_rating.copy() \n",
    "X = X[X>0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "\n",
    "test_ = pd.pivot_table(test,index=['review_profilename'],values=['review_overall'],\n",
    "               columns=['beer_beerid'])\n",
    "\n",
    "# Finding total non-NaN value\n",
    "total_non_nan = np.count_nonzero(~np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (sum(sum((test_ - y )**2))/total_non_nan)**0.5\n",
    "print('RMSE for Item-based model :  ',rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer  to 5): As can be seen, RMSE for User-User similarity is `1.84` and RMSE for Item-Item similarity is `2.26`. Hence User similarity model should get deployed in comparison to Item similarity as it has lesser RMSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)Determine how similar the first 10 users are to each other and visualise it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use SNS Clustermap to identify similarity between first 10 users. Please see the Dendrogram on the left. You can cut the dendrogram based on # similar clusters as needed** <br>\n",
    "\n",
    "**logic works as follows, since the final rating matrix obtained is a multi-index dataframe and cannot be traversed normally** <br>\n",
    "1) Iterate through user_final_rating dataframe for the ten rows, hence range (0,10) <br>\n",
    "2) Iterate through all the columns of dataframe and choose top 10 values (sort desc) <br>\n",
    "3) Append the values (including the name)to a list and then reshape the list <br>\n",
    "4) Change datatype of elements from Object to Float <br>\n",
    "5) Derive cluster map. Dendrograms show relativity between elements. Cut the dendrogram based on # of similar clusters needed! <br>\n",
    "\n",
    "\n",
    "**Using Correlation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_  = []\n",
    "for x in range(0, 10):\n",
    "    tmp = user_final_rating.iloc[x]\n",
    "    udf_.append(tmp.name)\n",
    "    for y in range(0,10): \n",
    "         udf_.append(tmp.sort_values(ascending=False)[y])\n",
    "            \n",
    "udf_ = pd.DataFrame(np.array(udf_).reshape(10,11))\n",
    "udf_.columns = ['review_profilename', '1','2','3','4','5','6','7','8','9','10']\n",
    "udf_ = udf_.set_index('review_profilename')\n",
    "udf_ = udf_.astype('float')\n",
    "#sns.clustermap(udf_ ,metric=\"correlation\",  cmap=\"mako\", col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single linkage clustering method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_  = []\n",
    "for x in range(0, 10):\n",
    "    tmp = user_final_rating.iloc[x]\n",
    "    udf_.append(tmp.name)\n",
    "    for y in range(0,10): \n",
    "         udf_.append(tmp.sort_values(ascending=False)[y])\n",
    "            \n",
    "udf_ = pd.DataFrame(np.array(udf_).reshape(10,11))\n",
    "udf_.columns = ['review_profilename', '1','2','3','4','5','6','7','8','9','10']\n",
    "udf_ = udf_.set_index('review_profilename')\n",
    "udf_ = udf_.astype('float')\n",
    "#sns.clustermap(udf_ ,method=\"single\",  cmap=\"mako\", col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result can also be obtained by Correlation matrix and heatmap for the top 10 values of first 10 users. Rows numbered 1 to 10 depict first 10 users. colors indicate how closely the values are correlated / how similar the users are with each other for each of the top 10 values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "#sns.heatmap(udf_.corr(), annot=True, linewidths=.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)Compute and visualise the similarity between the first 10 beers.**\n",
    "\n",
    "**We use SNS Clustermap to identify similarity between first 10 beers. Please see the Dendrogram on the left. You can cut the dendrogram based on # similar clusters as needed**  \n",
    "**logic works as follows, since the final rating matrix obtained is a multi-index dataframe and cannot be traversed normally**<br>\n",
    "1) Iterate through user_final_rating dataframe for the ten rows, hence range (0,10)<br> \n",
    "2) Iterate through all the columns of dataframe and choose top 10 values (sort desc) <br>\n",
    "3) Append the values (including the name)to a list and then reshape the list <br>\n",
    "4) Change datatype of elements from Object to Float <br>\n",
    "5) Derive cluster map. Dendrograms show relativity between elements. Cut the dendrogram based on # of similar clusters needed! <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_  = []\n",
    "for x in range(0, 10):\n",
    "    tmp = item_final_rating.T.iloc[x]\n",
    "    idf_.append(tmp.name[1].astype(str))\n",
    "    for y in range(0,10): \n",
    "         idf_.append(tmp.sort_values(ascending=False)[y])\n",
    "            \n",
    "idf_ = pd.DataFrame(np.array(idf_).reshape(10,11))\n",
    "idf_.columns = ['beer_beerid', '1','2','3','4','5','6','7','8','9','10']\n",
    "idf_ = idf_.set_index('beer_beerid')\n",
    "idf_ = idf_.astype('float')\n",
    "#sns.clustermap(idf_,  cmap=\"mako\", col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result can also be obtained by Correlation matrix and heatmap for the top 10 values of first 10 beers. Rows numbered 1 to 10 depict first 10 rows /first 10 beers. colors indicate how closely the values are correlated / how similar the beers are with each other for each of the top 10 values of a beer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplots(figsize=(10,10))\n",
    "#sns.heatmap(idf_.corr(), annot=True, linewidths=.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)Give the names of the top 5 beers that you would recommend to the users 'cokes', 'genog' and 'giblet' using both the models.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation based on User-prediction model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the index positions:\n",
    "print ('Cokes index pos:', user_final_rating.index.get_loc('cokes'))\n",
    "print ('Genog index pos:', user_final_rating.index.get_loc('genog'))\n",
    "print ('Giblet index pos:',user_final_rating.index.get_loc('giblet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 predictions based on user similarity model\n",
    "print (user_final_rating.iloc[user_final_rating.index.get_loc('cokes')].sort_values(ascending=False)[0:5])\n",
    "print (user_final_rating.iloc[user_final_rating.index.get_loc('genog')].sort_values(ascending=False)[0:5])\n",
    "print (user_final_rating.iloc[user_final_rating.index.get_loc('giblet')].sort_values(ascending=False)[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation based on Item-prediction model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the index positions:\n",
    "print ('Cokes index pos:', item_final_rating.index.get_loc('cokes'))\n",
    "print ('Genog index pos:', item_final_rating.index.get_loc('genog'))\n",
    "print ('Giblet index pos:',item_final_rating.index.get_loc('giblet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 predictions based on item similarity model\n",
    "print (item_final_rating.iloc[item_final_rating.index.get_loc('cokes')].sort_values(ascending=False)[0:5])\n",
    "print (item_final_rating.iloc[item_final_rating.index.get_loc('genog')].sort_values(ascending=False)[0:5])\n",
    "print (item_final_rating.iloc[item_final_rating.index.get_loc('giblet')].sort_values(ascending=False)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
